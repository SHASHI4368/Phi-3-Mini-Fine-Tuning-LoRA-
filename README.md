# Phi-3 Mini Fine-Tuning (LoRA) ğŸš€  

This repository contains code and experiments for fine-tuning the **[microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)** model using **Parameter-Efficient Fine-Tuning (PEFT)** with **LoRA**.  
The project is designed for **instruction tuning** and generating **product advertisements** with natural language generation.  

---

## ğŸ“Œ Features
- âœ… Fine-tunes `Phi-3-mini` with **LoRA adapters**  
- âœ… Uses **Hugging Face Transformers** + **PEFT** for training  
- âœ… Dataset handling via **ğŸ¤— Datasets**  
- âœ… Experiment tracking with **MLflow**  
- âœ… Model export & inference pipeline with Hugging Face `pipeline`  
- âœ… API integration with **FastAPI**  

---

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ phi3_mini_4_instruct_model_fine_tune.ipynb   # Main training notebook
â”œâ”€â”€ requirements.txt                             # Dependencies
â”œâ”€â”€ README.md                                    # Project documentation
â””â”€â”€ /models                                      # Fine-tuned models & LoRA adapters
```

---

## âš™ï¸ Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>
pip install -r requirements.txt
```

---

## ğŸ“Š Training

Open the notebook:

```bash
jupyter notebook phi3_mini_4_instruct_model_fine_tune.ipynb
```

Key steps inside:
1. Load tokenizer & base model (`microsoft/Phi-3-mini-4k-instruct`)
2. Prepare dataset (JSON/CSV â†’ ğŸ¤— `Dataset`)
3. Apply **LoRA** via PEFT
4. Train with Hugging Face `Trainer`
5. Log metrics with **MLflow**
6. Save model & adapter  

---

## ğŸš€ Inference

After training, you can load the fine-tuned model for text generation:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel

base_model = "microsoft/Phi-3-mini-4k-instruct"
lora_model = "./models/phi3-product-ads-lora"

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(base_model)
model = PeftModel.from_pretrained(model, lora_model)

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

print(generator("Write an engaging ad for a smartwatch", max_new_tokens=100))
```

---

## ğŸŒ API Deployment (FastAPI)

You can serve the model with a **REST API**:

```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

Example `app.py` snippet:

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class ProductRequest(BaseModel):
    product_name: str
    product_description: str

@app.post("/generate-ad")
def generate_ad(request: ProductRequest):
    # Call your pipeline here
    return {"ad_text": f"Generated ad for {request.product_name}"}
```

---

## ğŸ“ˆ Experiment Tracking

Training runs are tracked with **MLflow**:

```bash
mlflow ui
```

Open [http://127.0.0.1:5000](http://127.0.0.1:5000) to view metrics, losses, and artifacts.

---

## ğŸ›  Requirements

- Python 3.10+
- PyTorch
- Hugging Face Transformers
- Datasets
- PEFT
- Accelerate
- MLflow
- FastAPI (for API deployment)

Install everything via:

```bash
pip install -r requirements.txt
```

---

## ğŸ“œ License

This project is licensed under the **MIT License**.  

---

## ğŸ™Œ Acknowledgements
- [Microsoft Phi-3](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)  
- [Hugging Face Transformers](https://github.com/huggingface/transformers)  
- [PEFT (LoRA)](https://github.com/huggingface/peft)  
- [MLflow](https://mlflow.org/)  
